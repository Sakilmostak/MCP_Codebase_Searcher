# PRD: MCP Codebase Searcher

## 1. Project Title
MCP Codebase Searcher

## 2. Introduction/Goals
This document outlines the plan for the MCP Codebase Searcher, a command-line tool designed to search within a given codebase and provide insights into where and how specific code elements are used.

**Primary Goals:**
*   Develop a tool that can be initiated in any codebase directory.
*   Implement an API/command to search for all instances of a given input string or pattern.
*   Generate a structured output document detailing each found instance, including its location.
*   Leverage a Large Language Model (LLM), specifically Google Gemini, to infer usage context and provide detailed elaborations.
*   Implement a second API/command to elaborate on the details of specific findings from the generated output using the LLM.

## 3. Core Components/Modules

*   **`EntryPoint/CLI`**:
    *   Handles command-line arguments (`argparse` or similar).
    *   Orchestrates calls to other modules.
    *   Manages user interaction and output.

*   **`FileScanner`**:
    *   Responsibility: Traverse the specified codebase directory.
    *   Functionality:
        *   Identify relevant files to search (e.g., text-based source code files).
        *   Exclude configured or default ignored directories (e.g., `.git`, `node_modules`, `__pycache__`) and file types (e.g., binaries, images).
        *   Provide a list of file paths to the `SearchEngine`.

*   **`SearchEngine`**:
    *   Responsibility: Perform the actual search within individual files.
    *   Functionality:
        *   Accept a file path and a search query (string or regex).
        *   Find all occurrences of the query within the file content.
        *   Return a list of matches, including line numbers and the matched text.

*   **`ContextAnalyzer`**:
    *   Responsibility: Analyze the context surrounding each found instance using an LLM.
    *   Functionality:
        *   For each match, extract a snippet of surrounding code.
        *   Utilize the Google Gemini API to analyze the snippet and infer its usage (e.g., "variable declaration and initialization with a database connection string", "function call to an authentication service", "class definition for a data model").
        *   The LLM will be prompted to provide a concise summary of the code's purpose in context.

*   **`ReportGenerator`**:
    *   Responsibility: Compile search results into a structured and human-readable output.
    *   Functionality:
        *   Take processed findings (file path, line number, matched text, context snippet, inferred usage) from the `SearchEngine` and `ContextAnalyzer`.
        *   Format the data into a chosen output format (e.g., Markdown, JSON).
        *   Save the report to a file or print to standard output.

*   **`Elaborator`**:
    *   Responsibility: Provide more detailed explanations for specific items in a previously generated search report using an LLM.
    *   Functionality:
        *   Take a specific item/entry from a search report.
        *   Potentially re-read a larger portion of the source file for broader context.
        *   Utilize the Google Gemini API with the finding's details (code snippet, surrounding lines, file context) to generate a comprehensive explanation of its role, potential implications, or relationships with other code parts.
        *   The LLM will be prompted to elaborate on the initially inferred usage, providing deeper insights.

## 4. Detailed API/Command Specifications

### 4.1. Search API/Command

*   **Command (Example):** `mcp_search --query "my_variable" --path "/path/to/codebase" --output "report.md"`
*   **Function Signature (Conceptual):** `search_codebase(query: str, codebase_path: str = ".", output_file: str | None = None, config: dict | None = None) -> dict`

*   **Inputs:**
    *   `query` (str): The string, keyword, or regular expression to search for.
    *   `codebase_path` (str): The root directory of the codebase to search. Defaults to the current working directory.
    *   `output_file` (str, optional): Path to save the generated report. If not provided, output might go to `stdout` or a default file.
    *   `config` (dict, optional): Configuration options (e.g., files/directories to exclude, search sensitivity).

*   **Process:**
    1.  The `EntryPoint` parses arguments.
    2.  `FileScanner` traverses `codebase_path`, respecting exclusions, and generates a list of files.
    3.  The tool authenticates with the Google Gemini API using a configured API key.
    4.  For each file:
        a.  `SearchEngine` reads the file and finds all lines matching `query`.
        b.  For each match:
            i.  `ContextAnalyzer` extracts the surrounding code snippet.
            ii. The snippet is sent to the Google Gemini API to infer its usage and generate a contextual summary.
            iii. Collects file path, line number, matched text, snippet, and LLM-generated inferred usage/summary.
    5.  `ReportGenerator` takes all collected findings and formats them into the specified output (e.g., Markdown or JSON).
    6.  The report is saved to `output_file` or printed.

*   **Output (Report Structure - Example JSON item):**
    ```json
    [
      {
        "id": "unique_id_1", // A unique identifier for this finding
        "file_path": "src/utils/helpers.py",
        "line_number": 42,
        "matched_text": "my_variable",
        "context_snippet": [
          "line 40: def some_function():",
          "line 41:   # TODO: Refactor this",
          "line 42:   my_variable = get_config_value()",
          "line 43:   if my_variable is None:",
          "line 44:     return default_value"
        ],
        "inferred_usage": "Variable 'my_variable' is assigned the result of the 'get_config_value()' function call. This variable is then checked for None before being potentially used as a return value or in further logic.", // Example of LLM-generated summary
        "language_type": "Python" // Detected or assumed language
      }
      // ... more items
    ]
    ```
    *   If Markdown, it would be a human-readable list of these findings.

### 4.2. Elaboration API/Command

*   **Command (Example):** `mcp_elaborate --report "report.json" --id "unique_id_1"`
*   **Function Signature (Conceptual):** `elaborate_finding(report_path: str, finding_id: str, config: dict | None = None) -> str`

*   **Inputs:**
    *   `report_path` (str): Path to the previously generated search report file (likely JSON for easy parsing).
    *   `finding_id` (str): The unique identifier of the specific finding in the report to elaborate on.
    *   `config` (dict, optional): Configuration options.

*   **Process:**
    1.  The `EntryPoint` parses arguments.
    2.  Load and parse the report from `report_path`.
    3.  Locate the specific finding using `finding_id`.
    4.  The tool authenticates with the Google Gemini API.
    5.  The `Elaborator` module takes this finding:
        a.  It may re-read the original source file (`file_path` from the finding) to get a wider view around the `line_number`.
        b.  It prepares a prompt for the Google Gemini API, including the code snippet, context, and the original inferred usage, asking for a detailed elaboration.
        c.  The Gemini API provides a more detailed textual explanation.
    6.  The elaboration is returned as a textual description.

*   **Output:**
    *   A string containing a detailed textual explanation of the selected finding's usage and context.

## 5. Data Structures & Formats

*   **Search Report:**
    *   **JSON:** Preferred for programmatic access by the `Elaborator`. Schema as defined in section 4.1.
    *   **Markdown:** For human readability. Would include sections for each finding: File, Line, Snippet, Inferred Usage.
*   **Configuration File (e.g., `.mcp_search_config.json` or `~/.config/mcp_search/config.json`):**
    *   `exclude_dirs`: `[".git", "node_modules", "dist", "build"]`
    *   `exclude_files`: `["*.log", "*.tmp"]`
    *   `exclude_extensions`: `[".exe", ".dll", ".so", ".o", ".png", ".jpg"]`
    *   `search_options`: `{ "case_sensitive": false, "match_whole_word": true }`
    *   `gemini_api_key`: (string, optional) Directly in config (less secure) or specify path to key file. Best practice: Use environment variable `GOOGLE_GEMINI_API_KEY`.

## 6. Key Technologies/Libraries (Potential)

*   **Python 3.x** as the primary language.
*   **Standard Library:**
    *   `os`: For file system traversal (`os.walk`), path manipulation.
    *   `re`: For regular expression-based searching.
    *   `argparse`: For command-line interface parsing.
    *   `json`: For handling JSON reports and configurations.
    *   `dotenv` (e.g., `python-dotenv`): For managing API keys via `.env` files.
*   **AI/LLM:**
    *   `google-generativeai`: The official Google Gemini SDK for Python.
*   **Third-party (Optional, for advanced features):**
    *   `glob` for more complex file pattern matching.
    *   Language-specific AST parsers (e.g., `ast` for Python, or libraries like `tree-sitter` if broader language support is desired for deeper analysis) for more accurate context understanding. This is a significant complexity increase.
    *   `rich` or `click` for enhanced CLI output and interactivity.

## 7. Workflow/User Interaction

1.  User installs the tool (e.g., via pip).
2.  User navigates to their codebase directory or specifies it.
3.  **To Search:**
    `mcp_search --query "DatabaseConnection" --output report.json`
    Or for regex:
    `mcp_search --query "import \w+\.models" --regex --output report.md`
4.  The tool processes and generates `report.json` or `report.md`.
5.  User reviews the report.
6.  **To Elaborate:**
    `mcp_elaborate --report report.json --id "some_unique_id_from_report"`
7.  The tool prints a detailed explanation for that specific finding.

## 8. Exclusions & Considerations

*   **Initial Scope:** Focus on text-based files. Binary files will be ignored.
*   **Performance:** For very large codebases, file traversal and reading can be slow. API calls to Gemini will add latency.
    *   Consider optimizations like batching requests to Gemini if multiple snippets need analysis.
    *   Efficient file reading.
    *   Potentially parallel processing of files (adds complexity).
*   **Accuracy of Context Analysis:** While significantly enhanced by Gemini, the quality of inference depends on the model's capabilities, prompt engineering, and the clarity of the code snippet.
*   **LLM Dependency & Costs:**
    *   The tool will depend on the availability and performance of the Google Gemini API.
    *   API usage may incur costs. Users should be aware of Google's pricing model.
    *   Implement clear mechanisms for API key management.
*   **Data Privacy & Security:**
    *   Code snippets will be sent to Google's servers for analysis. Users must be aware of this and comfortable with Google's data usage policies.
    *   Consider if any sensitive information might be present in the code snippets and if redaction is needed (though this complicates analysis).
*   **API Rate Limits & Error Handling:**
    *   Implement robust error handling for Gemini API calls (network issues, authentication failures, rate limits, content filtering).
    *   Respect API rate limits.
*   **Error Handling:** Robust error handling for file access issues, invalid queries, unparseable reports, etc.
*   **Configuration:** A clear way to configure default ignored paths/files, and importantly, the Gemini API key (prefer environment variables like `GOOGLE_GEMINI_API_KEY`).
*   **Security:** Be mindful if handling user-provided regex patterns; avoid ReDoS vulnerabilities if possible, though this tool is locally run.

## 9. Future Enhancements

*   **More Sophisticated Search:**
    *   Case sensitivity options.
    *   Whole word matching.
    *   Language-aware search (e.g., search for function definitions vs. calls).
*   **Interactive Mode:**
    *   An interactive prompt to navigate search results and request elaborations.
*   **Advanced Context Analysis:**
    *   Leverage ASTs for supported languages in conjunction with LLM analysis for even richer context.
    *   Allow users to provide custom prompts or personas for the LLM analysis to tailor results.
*   **Report Formats:** Support for HTML or other rich formats.
*   **IDE Integration:** Plugins for popular IDEs to run searches and view results directly.
*   **Incremental Search/Caching:** For faster subsequent searches in the same codebase.
*   **Cross-referencing:** Link a function call to its definition within the report.
*   **GUI:** A graphical user interface for easier interaction.
*   **Pre-commit Hook:** Integrate as a pre-commit hook to search for certain patterns. 